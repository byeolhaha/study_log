## 아피치 에이브로를 사용하기 위해
- 어디간 스키마를 저장해 두어야 하는데 이를 카프카에 저장할 수는 없는 노릇이다 사이즈가 크기 때문이다. 전체 스키마를 다 알아야 하기 때문에
- 그래서 스키마 레지스트리라 불리는 아키텍처 패턴을 사용
- 스키마 레지스트리는 아파치 카프의 일부가 아니고 여러 오픈소스의 구현체 중 하나를 선택해서 사용하면 된다
- 나의 경우 컨플루언트에서 만든 것을 사용한다.
- 핵심 아이디어 = 카프카에 데이터를 쓰기 위해 사용되는 모든 스키마를 레지스트리에 저장
  - 카프카에 쓰는 레코드에는 사용된 스키마의 고유 식별자만 심어주면 된다.
  - 이 컨슈머는 이 식별자를 사용해서 스키마 레지스트리에서 스키마를 가져와서 데이터를 역직렬화할 수 있다.
  - 여기서 중요한 것은 이 모든 작업이 주어진 객체를 직렬화하는 시리얼라이저와 객체로 복원하는 디시리얼라이저 내부에서 수행된다는 것
  - 카프카에 데이터를 쓰는 코드는 그저 다른 시리얼라이저를 사용하듯이 에이브로 시리얼라이저를 사용하면 된다.
## 도커 컨테이너로 띄우기
docker run -p 8081:8081 --network kafka_docker_kafka_network  \
    -e  SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=PLAINTEXT://kafka01:9092,kafka00:9092,kafka02:9092 \
    -e SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081 \
    -e SCHEMA_REGISTRY_DEBUG=true \
    -e SCHEMA_REGISTRY_HOST_NAME=schema-registry confluentinc/cp-schema-registry
## 스키마 생성
```
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"type\":\"record\",\"name\":\"Customer\",\"namespace\":\"com.example\",\"fields\":[{\"name\":\"customerID\",\"type\":\"int\"},{\"name\":\"customerName\",\"type\":\"string\"}]}"}' \                       
http://localhost:8081/subjects/my-avro-value/versions    
```
## 스키마 조회
```
curl -X GET http://localhost:8081/subjects 

["my-avro-value"]%
```
