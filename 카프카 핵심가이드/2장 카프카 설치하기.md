# 2장 카프카 설치하기
배우는 내용
- 아파치 주키퍼 설치(브로커의 메타 데이터를 저장)하는 방법
- 아파치 카프카를 설치하기 위한 기본적인 설정 옵션
- 브러커를 실행하는 데 적합한 하드웨어를 선택하는 기준
- 여러 개의 카프카 브로커를 하나의 클러스터로 구성하는 방법
- 카프카를 프로덕션 환경에서 사용할 때 알아 두어야 할 것들
## 2.1 환경 설정
- 운영체제 선택하기
  대체로 리눅스 환경을 권장
- 자바 설치하기
  주키퍼와 카프카 모두 OpenJDK 기반 자바 구현체에서 원활히 작동함
  최신 버전을 다운받을 것을 권장
- 주키퍼 설치하기
  >주키퍼란
  카프카 클러스터의 메타데이터와 컨수머 클라이언트에 대한 정보를 저장
  설정 정보 관리, 이름 부여, 분산 동기화, 그룹 서비스를 제공하는 중앙화된 서비스
  - 독립 실행 서버
  - 주키퍼 앙상블
    - 주키퍼는 고가용성을 보장하기 위해서 앙상블이라 불리는 클러스터 단위로 작동 = 주키퍼도 단일 장애 지점을 피하기 위해서 여러 대의 서버로 운영
    - 이 앙상블은 홀수 개의 서버를 가지도록 권장되며, 안의 서버들이 과반수이상 작동해야 하기 때문이다.
      또한 5개의 노드 크기를 고려하라고 한다. 그 이유는 하나의 서버의 설정을 변경하면 중지하고 다시 시작해야 하는데 2대 이상의 노드 정지를 받아낼 수 없다면 위험이 따르기 때문이라고 한다.
      그러나 9대 이상의 서버를 유지하는 것도 권장하지 않는다고 한다. 그래서 5,7대를 유지하고 옵저버 서버를 차라리 추가하라고 권장한다.
      나는 이 부분을 읽으면서 몇가지 궁금한 점이 발생했고 그리고 몽고 디비의 복제셋과 비슷하게 관리되어짐을 깨달았다.
      - 왜 9대 이상일 때 성능 저하가 발생할까?
       chatGPT에 따르면
       ```
        카프카는 리더 선출과 데이터 일관성 유지를 위해 합의 프로토콜(예: ZooKeeper)을 사용합니다. 노드 수가 많아질수록 합의 과정에서 모든 노드가 동일한 상태를 유지하려고 서로 간의 메시지를 주고받는 작업이 증가합니다. 이로 인해 다음과 같은 성능 저하가 발생할 수 있습니다:
        - 합의 지연: 노드가 많아질수록 모든 노드 간의 합의에 필요한 시간이 늘어납니다. 특히 장애가 발생했을 때, 새로운 리더를 선출하거나 복구하는 과정에서 지연이 더 커질 수 있습니다.
        - 통신 오버헤드 증가: 노드가 9대를 넘어서면, 각 노드가 서로 데이터를 동기화하고 일관성을 유지하기 위해 통신하는 횟수가 급격히 증가합니다. 이에 따라 네트워크 및 처리 오버헤드가 늘어나 성능이 떨어질 수 있습니다.
        따라서 카프카는 노드 수를 9대 이하로 유지하도록 권장하며, 필요시 클러스터를 분리해 여러 클러스터로 운영하는 방식을 추천하는 경우가 많습니다.
       ```
      - 왜 5대와 7대일까? 2개 이상의 노드 정지를 받아낼 수 없으면 왜 위험이 따르는가?
       ```
       1. 왜 5대나 7대를 유지하라는가?
       홀수 노드 사용의 장점: 카프카와 같은 분산 시스템은 과반수 합의에 따라 리더 선출과 클러스터 상태를 유지합니다. 홀수 개의 노드를 사용하면 과반수 결정이 명확해 합의 과정에서 효율적입니다.
       안정성과 성능의 균형:
       - 5대: 2대까지 장애를 허용할 수 있으므로 안정적이면서도 운영 비용을 최소화합니다.
       - 7대: 3대까지 장애 허용이 가능하여 더 높은 안정성을 제공합니다.
       - 장애 내성: 홀수 개의 노드를 사용함으로써 재시작이나 장애가 발생할 때도 과반수를 쉽게 유지할 수 있습니다. 예를 들어, 5대에서 2대가 중지돼도 과반수를 유지하지만, 6대처럼 짝수 개를 사용하면 3대가 중지되었을 때 과반수를 유지할 수 없어 더 불안정해집니다.
       2. 9대 이상 사용을 권장하지 않는 이유
       합의 프로토콜 성능 저하: 9대 이상의 노드가 있으면, 카프카의 리더 선출이나 데이터 일관성을 맞추기 위한 합의 과정에서 네트워크 통신과 검증 작업이 급격히 늘어납니다.
       - 오버헤드 증가: 모든 노드가 서로 상태를 확인하고 일관성을 유지하려고 할 때, 네트워크와 리소스 오버헤드가 커지며 성능 저하가 발생할 수 있습니다. 특히 장애가 발생했을 때 새로운 리더를 빠르게 선출하기 어려워집니다.
       - 효율적 관리의 한계: 9대 이상은 추가적인 관리와 네트워크 비용이 발생하며, 안정성 향상보다 성능 저하 위험이 커져 클러스터 관리가 어려워집니다.
      ```
## 2.2 카프카 설치하기
- 나는 도커 컴포즈 통해서 설치했다.
```yml
version: '3.8'

volumes:
  Kafka00:
    driver: local
  Kafka01:
    driver: local
  Kafka02:
    driver: local

networks:
  kafka_network:
    driver: bridge

services:
  # Zookeeper 3개 (앙상블)
  zookeeper-0:
    image: bitnami/zookeeper:3.9.2
    container_name: zookeeper-0
    ports:
      - 2181:2181
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper-0:2888:3888,zookeeper-1:2888:3888,zookeeper-2:2888:3888
      ALLOW_ANONYMOUS_LOGIN: yes
    networks:
      - kafka_network

  zookeeper-1:
    image: bitnami/zookeeper:3.9.2
    container_name: zookeeper-1
    ports:
      - 2182:2181
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_SERVERS: zookeeper-0:2888:3888,zookeeper-1:2888:3888,zookeeper-2:2888:3888
      ALLOW_ANONYMOUS_LOGIN: yes
    networks:
      - kafka_network

  zookeeper-2:
    image: bitnami/zookeeper:3.9.2
    container_name: zookeeper-2
    ports:
      - 2183:2181
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_SERVERS: zookeeper-0:2888:3888,zookeeper-1:2888:3888,zookeeper-2:2888:3888
      ALLOW_ANONYMOUS_LOGIN: yes
    networks:
      - kafka_network

  # Kafka 3개 브로커
  Kafka00Service:
    image: bitnami/kafka:3.7.0
    container_name: Kafka00Container
    ports:
      - '10000:9094'
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_BROKER_ID=0
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-0:2181,zookeeper-1:2181,zookeeper-2:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://Kafka00Service:9092,EXTERNAL://127.0.0.1:10000
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_CLUSTER_ID=5p3DaHv8SBqPAbW7L98j0w
    networks:
      - kafka_network
    volumes:
      - Kafka00:/bitnami/kafka/data

  Kafka01Service:
    image: bitnami/kafka:3.7.0
    container_name: Kafka01Container
    ports:
      - '10001:9094'
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-0:2181,zookeeper-1:2181,zookeeper-2:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://Kafka01Service:9092,EXTERNAL://127.0.0.1:10001
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_CLUSTER_ID=5p3DaHv8SBqPAbW7L98j0w
    networks:
      - kafka_network
    volumes:
      - Kafka01:/bitnami/kafka/data

  Kafka02Service:
    image: bitnami/kafka:3.7.0
    container_name: Kafka02Container
    ports:
      - '10002:9094'
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_BROKER_ID=2
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-0:2181,zookeeper-1:2181,zookeeper-2:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://Kafka02Service:9092,EXTERNAL://127.0.0.1:10002
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_CLUSTER_ID=5p3DaHv8SBqPAbW7L98j0w
    networks:
      - kafka_network
    volumes:
      - Kafka02:/bitnami/kafka/data

  # Kafka Web UI
  KafkaWebUiService:
    image: provectuslabs/kafka-ui:latest
    container_name: KafkaWebUiContainer
    ports:
      - '8080:8080'
    environment:
      - KAFKA_CLUSTERS_0_NAME=Local-Kafka-Cluster
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=Kafka00Service:9092,Kafka01Service:9092,Kafka02Service:9092
      - DYNAMIC_CONFIG_ENABLED=true
    networks:
      - kafka_network


```
- 3개의 주키퍼 앙상블을 만들었다.
- 주키퍼 앙상블 설정:
  - ZOOKEEPER_SERVER_ID: 각 노드에 고유한 ID를 설정 (1, 2, 3).
  - ZOOKEEPER_SERVERS: 전체 주키퍼 노드의 목록을 설정.
  - 2888:3888 포트는 주키퍼 노드 간 통신용 포트입니다.
- 각 노드의 포트:
  - zookeeper-0: 클라이언트 포트 2181.
  - zookeeper-1: 클라이언트 포트 2182.
  - zookeeper-2: 클라이언트 포트 2183.
- Kafka 설정:
  - KAFKA_CFG_ZOOKEEPER_CONNECT: 앙상블의 모든 주키퍼 노드 주소를 설정.
- Kafka-UI:Kafka 브로커 kafka-0를 사용하여 모니터링.
- 주키퍼 설정에 대한 각 설명
  1. image: bitnami/zookeeper:3.9.2
     Bitnami에서 제공하는 Zookeeper 3.9.2 버전의 Docker 이미지를 사용합니다.
  2. container_name: zookeeper-2 컨테이너의 이름을 zookeeper-2로 지정합니다. 이를 통해 관리 및 참조 시 명확하게 구분할 수 있습니다.
  3. ports: - 2183:2181
     - 2183: 호스트 머신(로컬 머신)의 포트입니다.
     - 2181: 주키퍼의 클라이언트 포트입니다.
     - 의미: 로컬 머신의 2183 포트를 zookeeper-2 컨테이너의 2181 포트에 매핑합니다.따라서 클라이언트는 localhost:2183을 통해 이 노드에 접근할 수 있습니다.
  4. environment 환경변수
     주키퍼 컨테이너가 실행될 때 필요한 설정들을 환경 변수로 제공합니다.
     - ZOOKEEPER_CLIENT_PORT: 2181 클라이언트가 이 노드에 접속하기 위한 포트 번호입니다. 기본값은 2181입니다.
     - ZOOKEEPER_SERVER_ID: 3 이 주키퍼 노드의 고유 ID입니다. 주키퍼 앙상블에서 각 노드는 ID를 통해 자신을 식별합니다. ID는 1부터 순차적으로 할당됩니다.
     - ZOOKEEPER_SERVERS 앙상블에 참여하는 모든 주키퍼 노드의 정보를 명시합니다.
       - 각 항목의 의미:
         - zookeeper-0:2888:3888
         - zookeeper-0: 노드 이름.
         - 2888: 노드 간 데이터 동기화 포트.
         - 3888: 리더 선출을 위한 투표 포트.
         - zookeeper-1:2888:3888: 두 번째 노드.
         - zookeeper-2:2888:3888: 세 번째 노드.
         - 요약: 주키퍼 노드들은 서로 이 포트들을 통해 통신하며 앙상블의 상태를 유지합니다.
     - ALLOW_ANONYMOUS_LOGIN: yes 익명 사용자의 접근을 허용합니다.테스트용으로 주로 사용되며, 프로덕션에서는 보안을 위해 설정을 변경해야 합니다.
- 토픽 생성
  ```
  docker exec -it Kafka00Container kafka-topics.sh \
  --create \
  --bootstrap-server Kafka00Service:9092 \
  --replication-factor 2 \
  --partitions 1 \
  --topic test-topic


  ```
- 메시지 생성
  ```
  docker exec -it Kafka00Container kafka-console-producer.sh \
  --bootstrap-server Kafka00Service:9092 \
  --topic test-topic
  ```
- 메시지 읽기
  ```
   docker exec -it Kafka01Container kafka-console-consumer.sh \
  --bootstrap-server Kafka01Service:9092 \
  --topic test-topic \
  --from-beginning
  ```
![image](https://github.com/user-attachments/assets/d0a017e5-d168-41f4-bfc2-6f0b7819b75c)

- 위 버전은 주키퍼를 사용하는 버전이며 버전이 업그레이드 되면서 메타 정보들을 카프카 자체에서 관리할 수 있도록 바뀌었다고 한다.
  - Zookeeper 모드: Kafka 클러스터의 메타데이터를 Zookeeper가 관리한다.
  - KRaft 모드: Kafka 자체가 메타데이터를 관리합니다. Zookeeper가 필요하지 않다. **Kafka 2.8.0+**에서 도입되었고, Kafka 3.x부터는 추천되는 방식이다.
  - KRaft모드는 아래와 같다.
```
networks:
  kafka_network:

volumes:
  Kafka00:
  Kafka01:
  Kafka02:

services:
  ### Kafka00
  kafka00:
    image: bitnami/kafka:3.7.0
    restart: unless-stopped
    container_name: kafka00
    ports:
      - '10000:9094'
    environment:
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      # KRaft settings
      - KAFKA_CFG_BROKER_ID=0
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_KRAFT_CLUSTER_ID=HsDBs9l6UUmQq7Y5E6bNlw
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka00:9093,1@kafka01:9093,2@kafka02:9093
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      # Listeners
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka00:9092,EXTERNAL://127.0.0.1:10000
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    networks:
      - kafka_network
    volumes:
      - "Kafka00:/bitnami/kafka"
  ### Kafka01
  kafka01:
    image: bitnami/kafka:3.7.0
    restart: unless-stopped
    container_name: kafka01
    ports:
      - '10001:9094'
    environment:
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      # KRaft settings
      - KAFKA_CFG_BROKER_ID=1
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_KRAFT_CLUSTER_ID=HsDBs9l6UUmQq7Y5E6bNlw
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka00:9093,1@kafka01:9093,2@kafka02:9093
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      # Listeners
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka01:9092,EXTERNAL://127.0.0.1:10001
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    networks:
      - kafka_network
    volumes:
      - "Kafka01:/bitnami/kafka"
  ## Kafka02
  kafka02:
    image: bitnami/kafka:3.7.0
    restart: unless-stopped
    container_name: kafka02
    ports:
      - '10002:9094'
    environment:
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      # KRaft settings
      - KAFKA_CFG_BROKER_ID=2
      - KAFKA_CFG_NODE_ID=2
      - KAFKA_KRAFT_CLUSTER_ID=HsDBs9l6UUmQq7Y5E6bNlw
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka00:9093,1@kafka01:9093,2@kafka02:9093
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      # Listeners
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka02:9092,EXTERNAL://127.0.0.1:10002
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    networks:
      - kafka_network
    volumes:
      - "Kafka02:/bitnami/kafka"

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    restart: unless-stopped
    container_name: kafka-ui
    ports:
      - '8080:8080'
    environment:
      - KAFKA_CLUSTERS_0_NAME=Local-Kraft-Cluster
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka00:9092,kafka01:9092,kafka02:9092
      - DYNAMIC_CONFIG_ENABLED=true
      - KAFKA_CLUSTERS_0_AUDIT_TOPICAUDITENABLED=true
      - KAFKA_CLUSTERS_0_AUDIT_CONSOLEAUDITENABLED=true
      #- KAFKA_CLUSTERS_0_METRICS_PORT=9999
    depends_on:
      - kafka00
      - kafka01
      - kafka02
    networks:
      - kafka_network
  ```

## 2.3 브로커 설정하기
### 2.3.1 핵심 브로커 매개변수
```
  kafka-0:
    image: bitnami/kafka:3.7.0
    container_name: kafka-0
    ports:
      - 9094:9094
    depends_on:
      - zookeeper-0
      - zookeeper-1
      - zookeeper-2
    environment:
      ALLOW_PLAINTEXT_LISTENER: yes
      KAFKA_ENABLE_KRAFT: no
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper-0:2181,zookeeper-1:2181,zookeeper-2:2181
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,EXTERNAL://:9094
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka-0:9092,EXTERNAL://localhost:9094
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
```
- 일단 나는 책과 다르게 도커 컴포즈를 통해서 만들었기 때문에 이를 기준으로 정리하려고 한다. 하지만 이 내용은 브로커를 설치할 때 설정해야 하는 값이며 양식만 조금 다른거 같다.
- 브로커 ID
  - KAFKA_CFG_BROKER_ID를 이용해서 명시적으로 브로커 ID를 설정할 수 있으며 생략하는 경우 0부터 순차적으로 준다고 한다.
- 리스너
  > **Kafka에서 리스너(listener)** 는 클라이언트(프로듀서, 컨슈머) 또는 브로커(다른 Kafka 브로커 및 컨트롤러)가 Kafka 브로커와 통신하기 위해 사용하는 네트워크 엔드포인트.
쉽게 말해, **Kafka가 외부와 통신하기 위해 "열어놓은 문"**
  - 구버전은 단순히 포트 설정이지만 새로 도입된 버전에서는 쉼표로 구분된 리스터 이름과 URI 목록
  - 일반적인 보안 프로토콜이 아니라면 반드시 KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP 설정을 잡아주어야 한다.
  - 리스너는 {프로토콜}://{호스트이름}:{포트}의 형태
  - Kraft 모드 설정
    ```
          # Listeners
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka01:9092,EXTERNAL://127.0.0.1:10001
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
    ```
    - ALLOW_PLAINTEXT_LISTENER=yes
      - Kafka가 암호화 없이 PLAINTEXT(평문) 리스너를 허용하도록 설정합니다.
      - 보안 설정 없이 Kafka 브로커와 통신할 수 있습니다.
      - 운영 환경에서는 보안상 TLS(SSL) 등을 설정하는 것이 권장되지만, 개발/테스트 환경에서는 편의상 PLAINTEXT를 사용할 수 있습니다.
    - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - Kafka 브로커가 어떤 네트워크 인터페이스 및 포트에서 연결을 수락할지 지정하는 설정입니다.
      - PLAINTEXT://:9092 – 내부 브로커 및 클라이언트 통신용 (Kafka 클라이언트가 접속)
      - CONTROLLER://:9093 – 컨트롤러 통신을 위한 포트 (KRaft 모드에서 사용)
      - EXTERNAL://:9094 – 외부에서 Kafka에 접근할 수 있는 외부 클라이언트 통신 포트
      - 각 리스너는 다른 포트에서 서로 다른 목적으로 사용됩니다.
    - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka01:9092,EXTERNAL://127.0.0.1:10001
      - 클라이언트(예: Kafka 프로듀서, 컨슈머)가 브로커에 접근할 때 사용하는 호스트와 포트 정보를 클러스터에 알립니다.
      - PLAINTEXT://kafka01:9092 – 내부 네트워크에서 Kafka 브로커가 클러스터 내에서 사용할 주소
      - EXTERNAL://127.0.0.1:10001 – 외부 클라이언트가 localhost:10001로 Kafka에 접속할 수 있도록 설정
      - advertised(광고) 리스너는 클라이언트가 브로커에 접속할 때 사용해야 하는 호스트와 포트를 정의합니다.
    - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - 각 리스너에서 사용할 보안 프로토콜을 매핑하는 설정입니다.
      - CONTROLLER:PLAINTEXT – 컨트롤러 통신은 평문(PLAINTEXT)으로 수행
      - EXTERNAL:PLAINTEXT – 외부 클라이언트도 평문으로 통신
      - PLAINTEXT:PLAINTEXT – 내부 브로커 및 클라이언트 통신도 평문으로 설정
    - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - Kafka 브로커가 컨트롤러 역할을 수행할 때 사용하는 리스너를 지정합니다.
      - CONTROLLER 리스너(9093 포트)를 통해 Kafka 컨트롤러로서 동작하며, 다른 브로커와 클러스터 메타데이터를 주고받습니다.
    - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - Kafka 브로커 간의 통신(클러스터 내부 통신)에 사용할 리스너를 지정합니다.
      - PLAINTEXT 리스너(9092 포트)를 통해 브로커 간 통신을 수행합니다.
- zookeeper.connect
  - 구버전에 주키퍼가 필요할 때 해당 값을 설정해줬는데 Kraft모드에서는 사라진 설정
- log.dirs
  - 카프카는 모든 메시지를 로그 세그먼트 단위로 묶어서 log.dir 설정에 지정된 디스크 디렉토리에 저장
  - 다수의 디렉토리를 지정하고자 하는 경우 , log.dirs를 사용하자
  - 이 설정을 하지 않는 경우, log.dir가 사용되며 나의 경우 설정을 안해줘서 살펴보니 아래와 같이 기본 설정으로 된 거 같다.
    ![image](https://github.com/user-attachments/assets/a015ffc0-2288-49b4-90e9-cedba8a17130)
  - KAFKA_CFG_LOG_DIRS=/bitnami/kafka/logs
    - 위와 같이 설정 가능하며 쉼표로 구분해서 경로 목록을 만들어도 된다.
    - 1개 이상의 경로가 지정되었을 경우, 브로커는 가장 적은 수의 파티션이 저장된 디렉토리에 새 파티션을 저장한다.
    - 같은 파티션에 속하는 로그 세그먼트는 동일한 경로에 저장된다.
    - 사용된 디스크 용량 기준이 아닌 저장된 파티션 수 기준으로 새 파티션이 저장된 디렉토리 위치를 배정한다!!!!
    - 그래서 다수의 디렉토리에 대해 균등한 양의 데이터가 저장되지 않는다.
- num.recovery.threads.per.data.dir (KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR=4)
  - 카프카는 설정 가능한 스레드 풀을 사용해서 로그 세그먼트를 관리한다.
  - 아래 작업을 수행한다.
    - 브로커가 정상 시작되었을 때 로그 세그먼트 파일을 연다.
    - 장애가 발생하고 다시 시작하면, 각 파티션의 로그 세트먼트 파일을 검사하고 잘못된 부분 삭제
    - 브로커가 종료되었을 때 로그 세그먼트를 정상적으로 닫는다.
  - 기본적으로 하나의 로그 디렉토리에 대해 하나의 스레드만 사용
      ![image](https://github.com/user-attachments/assets/5cbd32f1-0a81-4eaa-84a3-5008eedc8d88)
  - 만약에 하나의 스레드만 사용되고 브로커에 500개의 파티션이 있고 3개의 로그 디렉토리가 있다면 브로커가 언클린 셧다운이 되었을 때
      장애를 복구하기 위해서 하나의 스레드가 직렬적으로 복구를 수행한다. 하지만 예를 들어 4개의 스레드가 배정되어 있다면 이 4개의 스레드가 병렬적으로 복구를 수행할 수 있다.
  - 따라서 로그 디렉토리에 배정된 스레드의 수에 따라 복구 시간이 길어지냐 짧아지냐가 결정되게 되며, 너무 많으면 네트워크 I/O가 많아져 병목이 심해지고 CPU사용량을 급격하게 많아져 오히려 느려질 수 있으니 본인의 서버 사양에 따라 테스트를 진행하여 설정해야 한다.
- auto.create.topics.enalbe (KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE)
  - 기본 설정에는 아래와 같은 상황에서 브로커가 토픽을 자동으로 생성한다. = kafka에서 토픽이 존재하지 않더라도 자동으로 생성하는 기능
    - 프로듀셔가 토픽에 메시지를 쓰기 시작할 때
    - 컨슈머가 프로듀서로부터 메시지를 읽기 시작할 때
    - 클라이언트가 토픽에 대한 메타데이터를 요청할 때
  - 하지만 이는 바람직하지 않다. 어떤 상황인지는 챗지피티를 통해 아래와 같이 정리해본다.
    1. 오타나 잘못된 설정으로 인한 토픽 생성
       - 만약 프로듀서가 event-log 대신 evnt-log라는 잘못된 토픽에 메시지를 보내면 Kafka는 evnt-log라는 토픽을 자동으로 생성합니다.
       - 개발자는 잘못된 토픽이 생성된 것을 인지하지 못하고, 이후 데이터가 잘못된 토픽으로 흘러 들어갑니다. 이로 인해 데이터 유실 및 분석 오류가 발생할 수 있습니다.
    2. 불필요한 토픽 증가
       - 컨슈머나 클라이언트가 잘못된 메타데이터 요청을 보내면 의미 없는 빈 토픽들이 무분별하게 생성됩니다. 시스템 자원이 낭비되고, 관리해야 할 토픽 수가 불필요하게 증가합니다.
    3. 토픽 설정 불일치
       - 자동 생성된 토픽은 기본 설정을 사용하므로, 복제 인수(replication factor)나 파티션 수가 적절하지 않을 수 있습니다. Kafka 클러스터의 데이터 안정성 및 부하 분산에 악영향을 줄 수 있습니다.
- auto.leader.rebalance.enable
  - 모든 토픽의 리더 역할이 하나의 브로커에 집중됨으로써 카프카 클러스터의 균형이 깨지는 수가 있다.
  - 이 설정을 활성화하면 가능한 한 리더 역할이 균등하게 분삼되도록 함
  - 파티션의 분포 상태를 주기적으로 확인하는 백그라운드 스레드가 시작되며 주기는 leader.imbalance.check.interval.seconds 값으로 설정한다.
  - 만약에 전체 파티션 중에 특정 브로커에 리더 역할이 할당된 파티션의 비율이 leader.imbalance.per.broker.percentage에 설정된 값이 넘어가면 파티션의 선호 리더 리밸런싱이 발생한다.
  - 설정은 아래와 같으며 클러스터 안에 여러 개의 브로커가 존재하는 경우 브로커마다 설정해준다.
    - KAFKA_CFG_AUTO_LEADER_REBALANCE_ENABLE=true
    - KAFKA_CFG_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS=300
    - KAFKA_CFG_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE=10
- delete.topic.enable (KAFKA_CFG_DELETE_TOPIC_ENABLE=true)
  - 클러스터의 토픽을 임의로 삭제하지 못하게끔 막아야할 때가 있을 때 false로 설정
### 2.3.2 토픽별 기본값
- 카프카 브로커 설정은 새로 생성되는 토픽에 적용되는 수많은 설정의 기본값 역시 지정한다.
1. num.partitions
 - 새로운 토픽이 생성될 때 몇 개의 파티션을 갖게 되는지를 결정
 - 주로 자동 토픽 생성이 활성화되어 있을 때 사용된다. 기본값은 1이다.
 - 토픽의 파티션 수는 늘릴 수만 있지 줄일 수는 없다!!!! 만약에 줄일려면 직접 토픽을 생성해야 한다.
 - 왜 줄일 수는 없는가를 지피티에 물어봤다.
   Kafka에서 파티션 수는 늘릴 수만 있고 줄이는 것은 불가능합니다.
   이는 Kafka의 데이터 분산 및 메시지 순서 보장 메커니즘 때문입니다.
   1. 데이터 일관성 문제
      - 파티션을 줄이면 기존 데이터의 일부가 유실되거나 잘못된 순서로 재배치될 위험이 있습니다.
      - Kafka는 파티션 내에서 메시지 순서를 보장합니다.
      - 파티션을 줄이면, 순서가 꼬이거나 메시지가 중복/누락될 수 있기 때문에 안전하지 않습니다.
   2. 리더 선출 문제
      - 각 파티션에는 리더 파티션이 존재하며, 컨슈머는 리더에서 데이터를 읽습니다.
      - 파티션을 줄이면 리더 파티션도 사라지게 되므로, 클러스터 안정성에 영향을 미칩니다.
   3. 해시 기반 파티셔닝
      - Kafka는 해시 기반 파티셔닝을 사용해 메시지를 특정 파티션에 할당합니다.
      - 파티션을 줄이면, 기존 해시 값과 매핑된 파티션이 사라지기 때문에 일관된 해시 기반 분산이 깨집니다.
- 토픽 자동 생성이 비활성화되어 있는 경우 아래와 같이 Kafka CLI를 사용해 직접 생성한다.
  - kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 2
- 브로커가 추가될 때 클러스터 전체에 걸쳐 메시지 부하가 고르게 분산되도록 파티션 개수를 잡아주는 것이 중요하다.
- 토픽당 파티션의 개수를 브로커의 수와 맞추거나 아니면 배수로 설정한다. 이렇게 하면 파티션이 브로커들 사이에 고르게 분산되도록 할 수 있다.
- 파티션 수는 어떻게 설정하는가?
  - 파티션은 많아야 하지만 그렇다고 해서 너무 많아서는 안된다.
  - 적절한 파티션의 수 = 토픽당 목표 처리량 / 컨슈머의 예상 처리량
 - 파티션이 많을수록 좋은 이유
   - 병렬 처리 증가: 각 파티션은 독립적인 데이터 스트림으로 동작합니다. 여러 컨슈머 그룹이 각 파티션을 병렬로 읽을 수 있기 때문에, 파티션이 많을수록 Kafka의 **처리량(throughput)**이 증가합니다.
   - 부하 분산: 데이터가 여러 파티션으로 분산되어 브로커의 부하가 균등하게 분산됩니다.
   - 확장성: 컨슈머 수가 많아질수록, 파티션이 충분해야 각 컨슈머가 파티션 하나씩을 담당하며, 처리 병목을 피할 수 있습니다.
- 파티션이 너무 많으면 발생하는 문제점
  - 메타데이터 오버헤드 증가
    - Kafka 클러스터는 모든 파티션에 대한 메타데이터(리더/팔로워 정보 등)를 유지해야 합니다.
    - 파티션이 많을수록 브로커 간의 메타데이터 동기화 부담이 커지고, 클러스터의 메모리 사용량이 증가합니다.
       예를 들어, 1,000개의 토픽에 각각 100개의 파티션이 있으면, 10만 개의 파티션 메타데이터를 관리해야 합니다.
       ➡️ 클러스터가 자주 불안정해지고, 메모리 부족으로 인해 장애가 발생할 수 있습니다.
  - 리더 선출 및 리밸런싱 속도 저하
    - Kafka 브로커 장애 시, 모든 파티션에 대해 새로운 리더 선출이 필요합니다.
    - 파티션 수가 많으면 리더 선출에 시간이 오래 걸리며, 서비스 지연이 발생할 수 있습니다.
    - 특히 KRaft 모드에서는 Raft 알고리즘이 동작하며, 많은 파티션이 있으면 합의(consensus)에 걸리는 시간이 길어집니다.
      ➡️ 장애 복구 시간이 길어지고, 클러스터가 불안정해질 수 있습니다.
  - 디스크 및 파일 핸들 오버헤드
    - Kafka는 각 파티션을 독립된 디렉터리와 파일 세트로 관리합니다.
    - 파티션이 많으면 파일 핸들(file handle) 사용량이 급증하고, 디스크 I/O가 증가합니다.
    - OS에서 열 수 있는 파일 핸들 수의 제한을 초과할 경우, 파일 열기 실패로 인해 Kafka가 제대로 동작하지 않을 수 있습니다.
      ➡️ 디스크 I/O 병목과 파일 핸들 부족으로 인한 장애 가능성 증가
  - 컨슈머 리밸런싱 지연
    - 컨슈머 그룹이 리밸런싱을 수행할 때, 각 파티션을 새롭게 할당합니다.
    - 파티션이 많으면 리밸런싱 시간이 길어지고, 서비스 지연 및 일시적인 데이터 읽기 중단이 발생합니다.
      ➡️ 장애 발생 시, 컨슈머 그룹 재할당이 느려져서 서비스 복구가 지연됩니다.
  - 불필요한 자원 낭비
    - 사용되지 않는 파티션이 많아지면, 브로커는 불필요한 자원(메모리, CPU, 디스크)을 소모하게 됩니다.
    - 파티션이 많아도 일부 파티션만 사용된다면 비효율적입니다.
2. default.replication.factor
  - 자동 토픽 생성 기능이 활성화되어 있을 경우, 이 설정은 새로 생성되는 토픽의 복제 팩터를 결정한다.
  - 복제 팩터값은 min.insync.replicas 설정값보다 최소 1이상 크게 잡아줄 것을 권장한다.
    - min.insync.replicas은 리더와 팔로워의 수를 합쳐서 수를 설정하는데 이는 이 수에 맞게 모두 복제가 성공되어야만 성공으로 간주한다.
    - 만약에 이 값이 1이면 리더에만 복제되어도 성공으로 간주하기 때문에 리더가 죽는 경우 팔로워들은 최신화된 데이터를 갖지 않을 수 있다. 팔로워의 상태를 확인하면 해당 개수를 만족하도록 함으로써 프로듀서가 메시지를 생성해도 되는 것인지 결정한다.
    - 만약에 해당 설정값이 2이고 복제셋이 3개라면 팔로워 2개가 모두 죽으면 프로듀서는 메시지를 생성하지 않는다고 한다.
  - 도커 컴포즈 kraft 모드에서의 설정은 아래와 같다.
    ```
    environment:
     - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false  # 자동 생성 비활성화
     - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=3     # 기본 복제본 수 설정
     - KAFKA_CFG_NUM_PARTITIONS=3                 # 기본 파티션 수 설정
    ```
3. log.retention.ms
 - 카프카가 얼마나 오랫동안 메시지를 보존할 것인지 지정
 - 단위가 시, 분, 초가 있는 작을수록 우선순위가 높기 때문에 ms로 설정하자
4. log.retention.bytes
 - 메시지 만료의 또 다른 기준은 보존되는 메시지의 용량
 - 파티션 단위로 결정되기 때문에 만약에 이 값이 1GB이고 파티션이 10개라면 토픽은 최대 10GB의 메시지를 가지게 된다.
 - 이 설정은 log.retention.ms와 같이 설정될 경우 조심해야 한다. 둘 중 한가지 조건만 성립해도 메시지는 삭제된다. 만약에 1GB가 되지도 않았는데 ms로 설정한 하루가 지나버리면 메시지는 삭제된다. 혹은 하루가 지나지도 않았는데 1GB가 꽉차 메시지가 삭제된다. 따라서 둘 중에 하나만 설정하기를 권장한다.
5. log.segment.bytes
   - 해당 설정은 로그 세그먼트의 크기를 설정한다.
   - 해당 크기에 도달하면 새로운 세그먼트를 연다. 따라서 크기를 너무 작게 설정하면 파일을 더 자주 열고 더 자주 닫는다.
   - 이 설정의 기본값은 1GB인데 하루에 들어오는 메시지 양이 100MB라면 해당 로그 세그먼트를 채우는데 10일 걸린다. log.retention.ms는 로그 세그먼트가 닫히고 나서의 설정 값이기 때문에 해당 설정값이 7일이라면 해당 메시지는 총 17일 동안 보존된다. 따라서 메시지가 뜸하게 만들어지는 경우 로그 세그먼트 크기를 조절하는 것이 중요할 수도 있다고 한다.
6. log.roll.ms
  - 로그 세그먼트의 닫히는 시간을 제어하는 방법
  - log.segment.bytes나 log.roll.ms 하나의 조건에 도달하는 경우 세그먼트를 닫는다.
  - 기본적으로 설정되어 있지 않기 때문에 크기 기준으로만 닫도록 되어져 있다.
  - 해당 값을 설정하면 다수의 파티션이 다수의 로그 세그먼트를 닫을 수 있기 때문에 디스크 성능에 영향을 줄 수있다고 한다.
7. min.insync.replicas
 - 앞서 토픽의 복제셋을 설정했던 배웠던 설정값인데 몇 개의 복제셋에 최신의 데이터를 가지고 있는 것은 성공으로 간주할 것인지를 설정하는 것이다.
 - 이것은 프로듀서의 ack 설정을 all로 잡아주는 것과 함께 사용된다고 한다.
 - 지속성을 높이기 위해 이 값을 올려 잡아 줄 경우 추가적인 오버헤드가 발생하면서 성능이 떨어질 수 있다고 한다. 따라서 몇 개의 메시지 유실 정도는 상간없고 높은 처리량을 받아내는 클러스터의 경우 이 설정값을 1로 설정하는 것을 권장한다고 한다.
   - 위 사례가 구체적으로 궁금했고 이해가 되지 않아 지피티에게 물어봤다. 아래와 같다.
      - min.insync.replicas 설정은 Kafka에서 메시지를 성공적으로 기록하기 위해 필요한 최소한의 동기화된 복제본(replica) 수를 지정합니다.
      - 높은 값 설정 (min.insync.replicas=2 이상)
        - 리더(Leader)뿐만 아니라 팔로워(Follower) 복제본도 최신 데이터를 가지고 있어야 메시지를 성공으로 간주합니다.
        - 팔로워가 리더와 동기화되지 않은 경우, 프로듀서 쓰기 실패가 발생합니다.
      - 낮은 값 설정 (min.insync.replicas=1)
        - 리더만 데이터 저장을 완료하면 성공으로 간주됩니다.
        - 팔로워 복제본이 늦거나 장애 상태여도 리더만 정상이라면 메시지가 기록됩니다.
      - 오버헤드가 발생하는 이유
        - 복제 비용 증가
          - min.insync.replicas가 리더 + 팔로워 2개로 설정된 경우:
            - 메시지가 리더뿐만 아니라 ISR에 있는 2개의 팔로워에도 동기화되어야 합니다.
            - 팔로워가 동기화되지 않으면 프로듀서는 계속 재시도합니다.
            - 결과적으로, 네트워크와 디스크 I/O가 증가하고 Kafka 브로커의 부하가 커집니다.
          - 지연(Latency) 증가
            - 모든 복제본이 메시지를 기록하고 동기화가 완료될 때까지 프로듀서가 응답을 기다려야 합니다.
            - 복제본 수가 많을수록 동기화 시간이 길어지고 지연이 발생합니다.
          - 팔로워 상태 불일치
            - 팔로워 복제본 중 하나라도 동기화되지 않은 경우, 프로듀서는 메시지를 쓰지 못하고 실패합니다.
            - 장애가 잦은 환경에서는 팔로워가 ISR에서 자주 빠지며, 그로 인해 성능 저하가 발생할 수 있습니다.
      - 메시지 유실을 감수하는 경우: 높은 처리량 우선
         - min.insync.replicas=1로 설정하면, 리더만 메시지를 저장해도 성공으로 처리됩니다. 팔로워 복제본이 동기화되지 않더라도, 빠르게 데이터를 기록하고 다음 요청을 처리할 수 있습니다.
         - ➡️ 결과: **처리량(throughput)** 이 증가하지만, 리더 장애 시 동기화되지 않은 데이터는 유실됩니다.
      - 유실을 감수해도 되는 상황 예시
        1. 실시간 로그 수집 시스템 (ELK, Kafka Log Shipper 등) 로그 데이터는 지속적으로 수집되며, 일부 데이터 유실은 허용 가능합니다.
        2. 사용자 행동 데이터 (Clickstream, 이벤트 로그) 사용자의 클릭이나 앱 내 행동 데이터는 실시간으로 수집되며 일부 누락이 발생해도 문제 없음.
        3. IoT 센서 데이터 (주기적인 상태 전송) IoT 기기에서 주기적으로 데이터를 전송하는 경우, 일부 데이터가 유실되어도 다음 데이터 전송으로 복구 가능합니다.
      - 유실을 허용하지 않는 경우: 금융, 트랜잭션 데이터
        1. 금융 거래 시스템 : 금융 거래 데이터는 유실이 발생하면 심각한 문제가 발생합니다.
8. message.max.bytes
   - 카프카 브로커는 쓸 수 있는 메시지의 최대 크기를 제한한다.
   - 기본값은 1,000,000 byte = 1MB이다
   - 프로듀서가 더 큰 크기의 메시지를 보내면 거부하고 에러를 리턴
   - 이는 압축된 데이터를 기반으로 하기 때문에 실제로 프로듀서에서 생성하는 메시지는 이보다 크다.
   - 해당 프로듀서가 메시지 크기를 알맞게 준다고 해도 컨슈머에 설정된 fetch.message.max.bytes 설정보다 크면 컨슈머는 메시지를 읽는 데 실패한다.
## 2.4 하드웨어 선택하기
- 성능을 고려한다면 디스크 처리량, 용량, 메모리, 네트워크, cpu를 감안해야 한다.
- 카프카를 매우 크게 확장할 경우, 업데이트되어야 하는 메타데이터의 양 때문에 하나의 브로커가 처리할 수 있는 파티션의 수에도 제한이 생길 수 있다.
### 2.4.1 디스크 처리량
